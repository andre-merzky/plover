
    # vim: set tw=64 nocindent

  - keep parts of the instance_data read_only, so that one
    adaptor cannot temper with instance data, and spoil init for
    other adaptors.  Only state related instance data (file
    pointer, job id, etc) should be mutable.  Define those, also
    for advert publishing.

  - different sets of instance data:
    instance_data, adaptor_instance_data, suite_instance_data
    (why limit to a adaptor suite?)

  - provide adaptor programmers with status check and monitoring
    threads automatically, and take care of thread resource
    management.

  - all objects should have a mutex, and lock it in non-const
    functions.  kind of.

  - allow an adaptor to statically bind, i.e. to bypass adaptor
    selection on following calls.  That improves performance
    for, for example, the message API, where the same channel
    has to be used anyway

  - have a hook in adaptor which gets called before/after any
    method call to the adaptor, e.g. to refresh state, caches,
    etc.  Possibly, also allow for hooks and shared 'instance'
    data for complete adaptor suites (see ini for aws adaptors)
    - ???

  - also have hooks in the packages, to do, for example, simple
    parameter checks, state checks, after-call-postprocessing,  etc.

  - allow for CPI and adaptor classes which do NOT have
    a session (buffer, context, ...)

  - sometimes, all adaptors which do _not_ throw win.  For
    example, on service discovery, but also if a session tries
    to get it's default contexts (set type to any, and call
    set_defaults())

  - ini thingies
    simplify, prune functionality

  - Sometimes, SAGA class implementation get used with adaptors.
    For example, a context may get used by other adaptors in the
    same suite.  Find a way to make it simple to get that
    implementation, or to add 'friend'ly methods to the saga
    class (latter is best if, e.g., "inherited from friend
    'cpi'" is sufficient).

  - provide means to do adaptor loop detection.  For example,
    allow to use SAGA for file staging from within the job
    adaptor, but don't allow that staging to run a job via the
    same adaptor.
    - low priority

  - add some utilities, like for wrapping a SAGA file into a 
    std::iostream
    - goes together with adaptor writers guide
    - needs to be documented along with the SPI

  - attributes interface should act like a string map
    job_description jd;
    std::map <std::string, std:string> attributes;
    jd["Executable"] = "/bin/date";

  - empty class ctors should do nothing, and in particular
    should not do remote ops.  Its braindead that a application
    class, which has a set of saga objects as mempers, needst to
    either initialize _all_ of them at creation time, via member
    initialization, or needs to wait for the default
    initialization, only to assignt and initialize them later
    again.  Sure, application can use pointers - but that kind
    of defeats our whole pimpl idea, doesn't it?  So, we need
    some mechanism to delay instantiation.  That _may_ go along
    well with async object creation btw.

    saga_file_factory.create_file <Async> (url);

    saga::file::file f (false);
    f.init <Async> (url);
    f.close ();

  - saga-a has predefined names for attributes.  Such names should 
    also exist for predefined metrics.

  - flags can simply be public const int members of the respective 
    classes.  We use int's anyway, no need to keep enums around in 
    the parent namespace then.


 - encapsulate adaptor selection
   probably too complex, and not worth it


  - saga::util is public visible / usable, and should be part of
    the language bindings

  - classes should inherit from a 'shareable', which keeps a weak 
    pointer of the class if the class is ever used for a shared ptr.
    No other shared_ptr can then ever claim that class, and the class
    gets a shared_from_this().

  - a task adaptor should be able to manage a thread pool, and call 
    into the sync adaptors (again via the engine?)

  - a task container adaptor should be able to group asks in a 
    container, and again to call back to bulk adaptors (via the engine?).
    task_container adaptors need to be called one after the other.  
    task::instance_data need to keep track of what adaptor runs what 
    task, etc

  - three call modes for adaptors:
    - simple:      call one and only one adaptor (bound functor)
    - trial/error: call one, on error call next, on success return (file.copy)
    - collect:     call all, collect results (service discovery)
    - all:         call one after the other, operating on the same functor
                   (think task_container)
    - make these modes exchangable, and allow each call to specify what is
      needed
    - collect: split collect-mode functor into [n] async bound functors, 
      and call templetized collector class to combine results

      result_combiner <service_description> (vector <vector <service_description> > sds)

  - everything should be async:
    - impl provides only async funcs - the api can make sync versions out of
      those.
    - if the adaptor provides async funcs, fine.  If it provides sync ones, the
      impl or functor or ... wraps them into threads (some task adaptor does
      that, right?).

  - for all nontempletized classes: only setters/getters/inspectors are inlined
    into header files - everything else (spanning more than one simple assign
    snd return statement) goes into the cpp files.


  - below API level, we express call modes as flags, in order to avoid
    templatization throughout the impl and cpi layers.  An exception might
    be the get_result templatization of saga::impl::task

  - ? allow for generic adaptors with the CPI

      void catchall (call_context cc);

    which get chain_called for each function call, for logging, timing, call
    transformation, ...  That adaptor should be able to say 'apply first' or
    'apply last' etc.

    That would make a threading async adaptor trivial - it just spawns a thread,
    and switches the cc's call_mode to sync before handin it off to the
    thread...


  
